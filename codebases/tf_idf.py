############################################################
##    FILENAME:   tf_idf.py    
##    VERSION:    1.0
##    SINCE:      2014-03-01
##    AUTHOR: 
##        Jimmy Lin (xl5224) - JimmyLin@utexas.edu  
##
############################################################
##    Edited by MacVim
##    Documentation auto-generated by Snippet 
############################################################

import nltk.corpus
from nltk.corpus import stopwords
from nltk.text import TextCollection
from nltk.text import Text
import heapq

def createTextFromTxtFile (fn):
    '''    
    Create text object from external text file
    Input: 
        fn - name of the text file
    Output:
        text 
    '''
    f = open (fn, 'rU')
    raw = f.read()
    tokens = nltk.word_tokenize(raw)
    text = nltk.Text(tokens)
    return text

def getTextFromString (textString, removeStopwords=True):
    '''
    Get text from the input string
    Input:
        textString - text string
        removeStopwords - specify whether to remove stop words
    Output:
        text - Text object containing the input string
    '''
    stop = stopwords.words('english')
    tokens = nltk.word_tokenize(textString)
    if removeStopwords:
        ptokens = [i for i in tokens if i not in stop]
    text = nltk.Text(ptokens)
    return text, ptokens

def getTextCollectionFromTxtFile (fn):
    '''
    Create text collection from external text files
    Input:
        fn - name of the external text file
    Output:
        textCollection containing all texts in the given file
    '''
    f = open (fn, 'rU')
    tc = []
    alltokens = []
    for line in f:
        text, tokens = getTextFromString(line)
        tc.append(text)
        alltokens.extend(tokens)
    return tc, alltokens

def unique (tokens):
    '''
    deduplicate the tokens
    '''
    nbefore = len(tokens)
    tokensAfterUniq = set(alltokens)
    nafter = len(tokensAfterUniq)
    return list(tokensAfterUniq), nbefore, nafter

def computeTF (term, text, alltexts):
    '''
    Compute the TF from text collection
    Input:
        term - 
        text
        alltexts - text collections
    Output:
        tf
    '''
    tf = alltexts.tf(term, text)
    return tf

class PriorityQueue:
    """
      Implements a priority queue data structure. Each inserted item
      has a priority associated with it and the client is usually interested
      in quick retrieval of the lowest-priority item in the queue. This
      data structure allows O(1) access to the lowest-priority item.

      Note that this PriorityQueue does not allow you to change the priority
      of an item.  However, you may insert the same item multiple times with
      different priorities.
    """
    def  __init__(self):
        self.heap = []
        self.count = 0

    def push(self, item, priority):
        entry = (priority, self.count, item)
        heapq.heappush(self.heap, entry)
        self.count += 1

    def pop(self):
        (_, _, item) = heapq.heappop(self.heap)
        return item

    def isEmpty(self):
        return len(self.heap) == 0

if __name__ == '__main__':
    alltexts, alltokens = getTextCollectionFromTxtFile('./so.txt')
    uniqtokens, nbefore, nafter = unique(alltokens)
    textCollection = TextCollection(alltexts)

    nTexts = len(alltexts)
    print "nTexts: ", nTexts

    pq = PriorityQueue()
    for term in uniqtokens:
        term_vector = []
        for ti in range(0, nTexts):
            text = alltexts[ti]
            tfidf = textCollection.tf_idf (term, text)
            term_vector.append(tfidf)
        summation = sum(term_vector) 
        pq.push((term, summation), summation)

    for i in range(0, pq.count):
        term, summation = pq.pop()
        print term, summation
