############################################################
##    FILENAME:   process_users.py    
##    VERSION:    1.5
##    SINCE:      2014-03-24
##    AUTHOR: 
##        Jimmy Lin (xl5224) - JimmyLin@utexas.edu  
##
############################################################
##    Edited by MacVim
##    Documentation auto-generated by Snippet 
############################################################

import csv, sys
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem.isri import ISRIStemmer
from nltk.corpus import words
from vectorizeTexts import unique

WORDS = set(words.words())

def preprocessing (major):
    # preprocessing
    major = major.replace("&", " ")
    major = major.replace(".", " ")
    major = major.replace("/", " ")
    lowcase_major = major.lower()
    
    separated_majors = lowcase_major.split(" ")
    return separated_majors

def vectorize_major(major_dict):
    st = LancasterStemmer()
    major_list = []
    for major in major_dict.keys():
        separated_majors = preprocessing(major)
        for i in range(len(separated_majors)):
            if separated_majors[i] in WORDS:
                if len(separated_majors[i]) <= 3:
                    continue
                major_list.append(st.stem(separated_majors[i]))

    uniqtokens, nbefore, nafter = unique(major_list)

    final_dict = {}
    index = 0
    for major in uniqtokens:
        if final_dict.has_key(major):
            pass
        else:
            final_dict.update({major:index})
            index += 1
     
    return final_dict

def usage():
    ustr = '''
    Description: 
       generate user features (matrix X) for IMC problem
    List of raw user features:
        0. UserID
        1. WindowID
        2. Split
        3. City 
        4. State
        5. Country
        6. ZipCode
        7. DegreeType
        8. Major
        9. GraduationDate
        10. WorkHistoryCount
        11. TotalYearsExperience
        12. CurrentlyEmployed
        13. ManagedOthers
        14. ManagedHowMany
    List of Output files:
       user_feat_out - matrix X
       log - statistics about each feature
    Usage: 
       process_users.py [user_raw_in] [train_user_id_in] [test_user_id_in]
    '''
    sys.stderr.write(ustr + '\n')


NARGS = 4
if __name__ == '__main__':
    # validate the input
    nargs = len(sys.argv)
    if nargs != NARGS:
        usage()
        sys.exit(-1)
    
    rawUsersIn = open(sys.argv[1],'rb')
    train_userIndexIn = open(sys.argv[2], 'rb')
    test_userIndexIn = open(sys.argv[3], 'rb')
    train_userFeatureOut = open("train_X", 'wb')
    test_userFeatureOut = open("test_X", 'wb')
    log = open('process_users.log', 'wb')

    original = sys.stdout
    sys.stdout = log
    rawUsersIn = csv.reader(rawUsersIn, delimiter='\t')
    train_userFeatureOut = csv.writer(train_userFeatureOut, delimiter=' ')
    test_userFeatureOut = csv.writer(test_userFeatureOut, delimiter=' ')
    header = rawUsersIn.next()
    nColumns = len(header)

    #-------------------------------------------------------------
    train_users_id_lookup = {}
    pos = 0
    for user_id_str in train_userIndexIn:
        user_id = int(user_id_str)
        train_users_id_lookup.update({user_id:pos})
        pos += 1
    test_users_id_lookup = {}
    pos = 0
    for user_id_str in test_userIndexIn:
        user_id = int(user_id_str)
        test_users_id_lookup.update({user_id:pos})
        pos += 1
    #-------------------------------------------------------------
    
    dictionaries = [] # dictionaries, each one contains lookup between number and value
    values = [] # restore counters of distinct label
    for i in range(0, nColumns):
        dictionaries.append({})
        values.append(0)

    ## DISCRITIZATION
    nominalSet = range(3,9) + range(12,14) # index of nominal attributes
    removeSet = [1,2,3,5,6,9] # index of attributes to remove
    remainedSet = list(set(range(0,len(header))).difference(set(removeSet)))
    remainedSet.sort()
    USER_INDEX_INCLUDED = True

    SCSUsers = [] # users matrix by simple coding scheme
    BCSUsers = [] # users matrix by binary coding scheme
    newheader = [header[i] for i in remainedSet]
    INDEX_IDX = 0
    MAJOR_IDX = 8

    for user in rawUsersIn:
        windowID = user[1]
        if not windowID == '1':
            break
        split = user[2]
        userid = user[0]
        for i in nominalSet:
            ## restore the MAJOR value
            key = user[i]
            if not dictionaries[i].has_key(key):
                dictionaries[i][key] = values[i]
                values[i] += 1
            if not i == MAJOR_IDX:
                user[i] = dictionaries[i][key]

        scsUser = user
        #print scsUser
        SCSUsers.append(scsUser)

    """
    STEP: MERGE MAJOR
    """
    major_keywords_dict = vectorize_major(dictionaries[MAJOR_IDX])
    values[MAJOR_IDX] = len(major_keywords_dict.keys())
    nSCSFeatures = len(SCSUsers[0])
    print "nSCSFeatures:", nSCSFeatures
    
    print "Domain size of each variables: "
    newheader = []
    for i in range(0, nColumns):
        if i in removeSet: continue
        if values[i] <= 0: values[i] = 1
        print "  ", header[i], values[i]
        newheader = newheader + [header[i]] * values[i]

    for item in major_keywords_dict.items():
        print item

    """
    STEP: output the accumulated counter
    """
    for i in range(nColumns):
        if i in removeSet: continue
        domainSize = len(dictionaries[i].items())
        if not (domainSize == 0):
            print '===========', header[i], '==========='
            items = dictionaries[i].items()
            if domainSize <= 300 or True:
                for key, value in items:
                    print key, ":::",  value

    sys.stdout = original

    
    '''
    ## HEADER FORMULATION
    userFeatureOut.writerows([newheader])
    '''
    nBCSFeatures = None
    st = LancasterStemmer()

    for scsUser in SCSUsers:
        acc_index = 0
        bcsUser = [] # new sparse user vector in binary coding scheme
        for i in range(nSCSFeatures):
            if i == INDEX_IDX:
                # output USERID in separate file: win1_Users.index
                # userIndexOut.writerows([[scsUser[i]]])
                if USER_INDEX_INCLUDED:
                    bcsUser.append(str(0)+":"+str(scsUser[i]))
                else:
                    bcsUser.append(str(0)+":"+str(0))
                acc_index += 1
                continue
            if i in removeSet: 
                # ignore feature to be removed
                continue
            if i in nominalSet: 
                # binary encoding for nominal attributes
                if i == MAJOR_IDX:
                    ## major representation
                    sep_majors = preprocessing(scsUser[MAJOR_IDX])
                    for sep_major in sep_majors:
                        word = st.stem(sep_major)
                        if major_keywords_dict.has_key(word):
                            index = acc_index + major_keywords_dict[word]
                            bcsUser.append(str(index)+":"+str(1))
                else:
                    index = acc_index + scsUser[i]
                    bcsUser.append(str(index)+":"+str(1))
                acc_index += values[i]
            else: # non-norminal set
                if scsUser[i] > 0:
                    index = acc_index 
                    bcsUser.append(str(index)+":"+str(scsUser[i]))
                acc_index += 1
        ## display the binary encoding result and guarantee its safety
        if nBCSFeatures is None:
            nBCSFeatures = len(bcsUser)
            print "nBCSFeatures (total):", nBCSFeatures
        ## output to external csv, piece by piece
        if train_users_id_lookup.has_key(int(scsUser[INDEX_IDX])):
            train_userFeatureOut.writerows([bcsUser])
        if test_users_id_lookup.has_key(int(scsUser[INDEX_IDX])):
            test_userFeatureOut.writerows([bcsUser])
