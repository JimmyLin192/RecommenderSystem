#!/usr/bin/python2.7
############################################################
##    FILENAME:   process_jobs.py    
##    VERSION:    1.2
##    SINCE:      2014-03-24
##    AUTHOR: 
##        Jimmy Lin (xl5224) - JimmyLin@utexas.edu  
##
############################################################
##    Edited by MacVim
##    Documentation auto-generated by Snippet 
############################################################

'''
TODO LIST:
    1. how to particularly deal with TITLE of job
    2. how to deal with DESCRIPTION and REQUIREMENT in distinct ways
'''

import sys,csv
import re
from util import *
from vectorizeTexts import *

csv.field_size_limit(sys.maxsize)

#TODO: use more elegant natural language method to extract useful words

inFile = './jobs1.tsv'
with open(inFile,'rb') as rawJobsIn,\
        open('./win1_Jobs.sparse', 'wb') as jobFeatureOut, \
        open('./win1_Jobs.index', 'wb') as jobIndexOut, \
        open('./keywords.conf', 'rb') as keywordConf:

    """
    STEP ONE: open external file for preparation
    """
    rawJobsIn = csv.reader(rawJobsIn, delimiter='\t')
    jobFeatureOut = csv.writer(jobFeatureOut)
    jobIndexOut = csv.writer(jobIndexOut)

    # process header
    header = rawJobsIn.next()
    nColumns = len(header)

    """
    STEP TWO: preparation for non-textual discretization
    """
    dictionaries = [5] # dictionary for discretizing each column
    values = [] # numerical count of distinct label
    for i in range(0, nColumns):
        dictionaries.append({})
        values.append(0)
    
    INVESTIGATE_ATTR_SET = [2, 5, 6, 7, 8]
    nSampledJobs = 0
    for row in rawJobsIn:
        nSampledJobs += 1
        for i in INVESTIGATE_ATTR_SET:
            if not dictionaries[i].has_key(row[i]):
                dictionaries[i][row[i]] = values[i]
                values[i] += 1
    print 'nInputJobs: ', nSampledJobs

    NOMINAL_ATTR_SET = [2] + range(5,9) # set of index of discrete attr
    print 'List of features: '
    for i in NOMINAL_ATTR_SET:
        print "\tdict_" + str(i) + ":"+ str(len(dictionaries[i].items()))
        print "\tvalue_" + str(i) + ":" + str(values[i])

    """
    STEP THREE: preparation for textual discretization
    """
    keywords = []
    for line in keywordConf:
        line = line.strip("\n")
        [kw, f, df] = line.split(" ")
        keywords.append(kw)
    nKeywords = len(keywords)
    print "nKeywords:", nKeywords

    """
    STEP FOUR: header processing
        0. JobID
        1. WindowID
        2. Title
        3. Description
        4. Requirement
        5. City
        6. State
        7. Country
        8. Zip5
        9. StartDate
        10. EndDate
    """
    '''
    header[-1] = header[-1].strip("\n")
    jobFeatureOut.writerows([[header[0]]+ [header[2]] + header[5:9] + header[11:] + \
                      ["kw:"+str(x) for x in keywords]])
    '''
    #NOMINAL_ATTR_SET = [] # set of index of discrete attr
    ID_INDEX = 0
    WINDOWID_IDX = 1
    DESCRIPTION_IDX = 3 # progress of job description
    
    """
    STEP FIVE: process job profile
    """
    # reopen the file for second pass
    rawJobsIn = open(inFile,'rb')
    rawJobsIn = csv.reader(rawJobsIn, delimiter='\t')
    pb = ProgressBar(nSampledJobs, 50)    
    progress = 0
    for row in rawJobsIn:
        progress += 1
        # NOTE: for now we only consider instance with win=1
        #if not row[WINDOWID_IDX] == '1':
        #    continue
        # output ID to process_jobs.index file
        jobIndexOut.writerows([[row[ID_INDEX]]])
        '''
        # output features to process_jobs.sparse file
        jobFeature = []
        acc_index = 0
        # for non-textual feature
        for i in NOMINAL_ATTR_SET:
            if not dictionaries[i].has_key(row[i]):
                print "nokey:", row[i]
                continue
            # add to sparse feature matrix
            jobFeature.append(str(acc_index+dictionaries[i][row[i]])+":1")
            acc_index += values[i] 

        # for textual feature (append in the last part)
        description = row[DESCRIPTION_IDX]
        tokens = nltk.word_tokenize(description)
        text = nltk.Text(tokens)
        text.tokens = processTokens(text.tokens)
        for i in range(0, nKeywords):
            count = text.count(keywords[i])
            if count >= 1:
                jobFeature.append(str(acc_index+i)+":1")

        # write to out file
        jobFeatureOut.writerows([jobFeature])
        '''

        # progress bar
        pb.update(progress)
        pb.display()
